{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cifar = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./cifar10', train=True, download=True, transform=transform_cifar)\n",
    "test_dataset = datasets.CIFAR10(root='./cifar10', train=False, download=True, transform=transform_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 128, shuffle= True, num_workers = 2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 128, shuffle= True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models teacher and student "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherNN(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # input (n, 3, h, w) -> (n, 128, h, w)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1), \n",
    "            nn.ReLU(), \n",
    "            # (n, 128, h, w) -> (n , 64, h , w)\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n, 64, h, w) -> (n , 64, h//2, w//2)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # (n , 64, h//2, w//2) -> (n , 64, h//2, w//2)\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(), \n",
    "            # (n , 64, h//2, w//2) -> (n , 32, h//2, w//2)\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            # (n , 32, h//2, w//2) => (n , 32, h//4, w//4)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 32  * 32//4  * 32//4\n",
    "            nn.Linear(2048, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentNN(nn.Module):\n",
    "    def __init__(self, num_classes:int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = nn.Adam(model.paremeters(), lr = learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x , y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss +=running_loss.item()\n",
    "        print(f\"{epoch}/{epochs} Loss: {running_loss}/{len(train_loader)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device) , y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, prediction = torch.max(outputs)\n",
    "            total += y.size(0)\n",
    "            correct += (prediction == y).sum().item()\n",
    "        accurecy = 100 * correct/total\n",
    "        \n",
    "        print(f\"Test Accurecy {accurecy:.2f}%\")\n",
    "    return accurecy            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_teacher = TeacherNN(num_classes=10).to(device)\n",
    "train(nn_teacher, train_loader=train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_teacher = test(nn_teacher, test_loader, device)\n",
    "torch.manual_seed(42)\n",
    "nn_student = StudentNN(num_classes = 10).to(device)\n",
    "disillited_student = StudentNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of the smaller network : 2.327361822128296\n",
      "norm of the distillited network : 2.327361822128296\n"
     ]
    }
   ],
   "source": [
    "print(f\"norm of the smaller network : {torch.norm(nn_student.features[0].weight).item()}\")\n",
    "print(f\"norm of the distillited network : {torch.norm(nn_student.features[0].weight).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of parameters in the teacher network 1,186,986\n",
      "num of parameters in the student network 267,738\n",
      "size of teacher compared to student 4.43\n"
     ]
    }
   ],
   "source": [
    "print(f\"num of parameters in the teacher network {sum( i.numel() for i in nn_teacher.parameters()):,}\")\n",
    "print(f\"num of parameters in the student network {sum( i.numel() for i in nn_student.parameters()):,}\")\n",
    "print(f\"size of teacher compared to student {sum( i.numel() for i in nn_teacher.parameters())/sum( i.numel() for i in nn_student.parameters()):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(nn_student, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_student_before_distillation = test(nn_student, test_loader=test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"teacher_test_accuracy {test_accuracy_teacher:0.2f}%\")\n",
    "print(f\"student_test_accuracy {test_accuracy_student_before_distillation:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response based distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kd(teacher:nn.modules, student:nn.modules, train_loader:DataLoader, epochs:int, learning_rate: float, temperature:float, soft_target_loss_weigh: float, ce_loss_weight: float, device = torch.device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    for i in range(epochs):\n",
    "        for x , y in train_loader:\n",
    "            running_loss = 0.0\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(x)\n",
    "            \n",
    "            student_logits = student(x)\n",
    "            \n",
    "            soft_targets = nn.functional.softmax(teacher_logits/temperature, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits/temperature, dim=-1)\n",
    "            \n",
    "            kl_divergence_loss = (torch.sum(soft_targets * (soft_targets.log() - soft_prob))/soft_prob.size()[0]) * temperature**2\n",
    "            label_loss = ce_loss(student_logits, y)\n",
    "            \n",
    "            loss = ce_loss_weight * label_loss + soft_target_loss_weigh * kl_divergence_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+= running_loss\n",
    "            \n",
    "        print(f\"epoch{i+1}/{epochs} loss:{loss/len(train_loader):.02f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kd(teacher=nn_teacher, student=disillited_student, train_loader=train_loader, epochs=10, learning_rate=0.001, temperature=2, soft_target_loss_weigh=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_student_after_distillation = test(disillited_student, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"teacher test accuracy {test_accuracy_teacher:0.2f}\")\n",
    "print(f\"student test accuracy {test_accuracy_student_before_distillation:0.2f}\")\n",
    "print(f\"distilled test accuracy {test_accuracy_student_after_distillation:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # input (n, 3, h, w) -> (n, 128, h, w)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n, 128, h, w) -> (n , 64, h , w)\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n, 64, h, w) -> (n , 64, h//2, w//2)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # (n , 64, h//2, w//2) -> (n , 64, h//2, w//2)\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n , 64, h//2, w//2) -> (n , 32, h//2, w//2)\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n , 32, h//2, w//2) => (n , 32, h//4, w//4)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 32  * 32//4  * 32//4\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flatten_features = torch.flatten(x, 1)\n",
    "        x = self.classifier(flatten_features)\n",
    "        # pool the flattened tensor\n",
    "        flatten_features = nn.functional.avg_pool1d(flatten_features, 2)\n",
    "        return x, flatten_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.features(x)\n",
    "        flatten_features = torch.flatten(x, 1)\n",
    "        x = self.classifier(flatten_features)\n",
    "\n",
    "        return x, flatten_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_hidden = TeacherNNCosine(10).to(device)\n",
    "teacher_hidden = teacher_hidden.load_state_dict(nn_teacher.state_dict())\n",
    "torch.manual_seed(42)\n",
    "student_hidden = StudentNNCosine(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cosine_loss(teacher:nn.Module, student:nn.Module, train_loader:DataLoader, epochs:int, learning_rate:float, hidden_rep_weight:float, ce_loss_weight:float, device:torch.device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    cosine_loss = nn.CosineEmbeddingLoss()\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for x , y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                _, teacher_hidden_state = teacher(x)\n",
    "            student_logits , student_hidden = student(x)\n",
    "            \n",
    "            rep_loss = cosine_loss(teacher_hidden_state , student_hidden, torch.ones_like(student_hidden.size()[0]).to(device))\n",
    "            \n",
    "            pred_loss = ce_loss(student_logits, y)\n",
    "            \n",
    "            loss = rep_loss * hidden_rep_weight + pred_loss * ce_loss_weight\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss +=loss.item()\n",
    "        print(f\"epoch {i+1}/{epochs} loss: {running_loss/len(train_loader):.02f}\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cosine_loss(model:nn.Module, test_loader:DataLoader, device:torch.device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = len(test_loader)\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs, _ = model(x)\n",
    "            confidence, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == y).sum().item() \n",
    "        accuracy = correct / total\n",
    "        print(f\"Test accuracy :{accuracy}\")\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cosine_loss(teacher=teacher_hidden, student=student_hidden, train_loader=train_loader, epochs=10, learning_rate=0.001, temperature=2, soft_target_loss_weigh=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_student_after_distillation = test_cosine_loss(student_hidden, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using intermediate regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherNNCosineRegressor(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # input (n, 3, h, w) -> (n, 128, h, w)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n, 128, h, w) -> (n , 64, h , w)\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n, 64, h, w) -> (n , 64, h//2, w//2)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # (n , 64, h//2, w//2) -> (n , 64, h//2, w//2)\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n , 64, h//2, w//2) -> (n , 32, h//2, w//2)\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # (n , 32, h//2, w//2) => (n , 32, h//4, w//4)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 32  * 32//4  * 32//4\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        hidden_rep = x\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x, hidden_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.regressor = nn.Conv2d(16,32, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.features(x)\n",
    "        regressed_hidden = self.regressor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x, regressed_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_hidden = TeacherNNCosine(10).to(device)\n",
    "teacher_hidden = teacher_hidden.load_state_dict(nn_teacher.state_dict())\n",
    "torch.manual_seed(42)\n",
    "student_hidden = StudentNNCosine(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressor(teacher:nn.Module, student:nn.Module, epochs:int, learning_rate:float, ce_loss_weight:float, hidden_rep_weight:float, device:torch.device):\n",
    "    \n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam()\n",
    "    teacher.to(device)\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    student.eval()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                _, hidden_rep_teacher = teacher(x)\n",
    "            student_logits, hidden_rep_student = student(x)\n",
    "            \n",
    "            rep_loss = mse_loss(hidden_rep_teacher, hidden_rep_student)\n",
    "            ce_loss = ce_loss(student_logits, y)\n",
    "            \n",
    "            loss = ce_loss_weight * ce_loss + hidden_rep_weight * rep_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item()\n",
    "        print(f\"Epoch {i+1}/{epochs} , loss: {running_loss/len(train_loader):0.02f}\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_regressor(model:nn.Module, test_loader:DataLoader, device:torch.device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for x, y in test_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        confidence, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size()[0]\n",
    "    \n",
    "    accuracy = correct/total\n",
    "    print(f\"Test accuracy: {accuracy:0.2f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regressor(teacher=teacher_hidden, student=student_hidden, train_loader=train_loader, epochs=10, learning_rate=0.001, temperature=2, soft_target_loss_weigh=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_student_after_distillation = test_regressor(student_hidden, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
